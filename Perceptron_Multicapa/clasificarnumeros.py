# -*- coding: utf-8 -*-
"""ClasificarNumeros.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W859nsj2cd7BId2PtpdChOdUl9azz8L2

# Clasificación de imágenes
"""

#librerias
import numpy as np
import matplotlib.pyplot as plt
from keras.datasets import mnist
#librerias Keras
from keras import Sequential
from keras import backend
from tensorflow.keras.utils import to_categorical
from keras import optimizers
from keras.layers import Dense
#guardar el modelo
from keras.callbacks import ModelCheckpoint
from sklearn.metrics import confusion_matrix

#obtener datos
(x_train,y_train),(x_test,y_test) = mnist.load_data()

print(x_train.shape)
print(x_test.shape)
print(y_test.shape)

y_test1=y_test

#visualizar los datos
plt.imshow(x_train[20000],cmap='gray')
plt.show()
print(y_train[20000])
print(x_train[20000])

#preprocesamiento de la imagen
  #llevar la matriz a un vector, normalizar datos (0-1)
  #10 clases (0-9), una neurona por clase, 1-hot para las salidas

x_train = x_train.reshape(60000,784).astype('float32') #matriz -> vector
x_test = x_test.reshape(10000,784).astype('float32') #matriz -> vector
#normalizar
x_train /= 255.0
x_test /= 255.0

print(x_train[10])

#1-hot a targets
y_train1 = to_categorical(y_train)
y_test1 = to_categorical(y_test)

print(y_train[2],y_train1[2])
backend.clear_session()

#modelo
modelo = Sequential() #Dense hace referencia a que todas las entradas tienen conectado un peso, se conecta todo vs todo
#capa
modelo.add(Dense(300, use_bias=True,activation='sigmoid', input_shape=(784,)))
#capa3
modelo.add(Dense(10, use_bias=True,activation='softmax')) #softmax el resultado de las operaciones exponenciales es igual a 1. Problemas de múltiples clases

#optimizador

Adam = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0.0)
modelo.compile(loss='categorical_crossentropy',optimizer = Adam, metrics = ['accuracy'])

M = modelo.fit(x_train,y_train1, epochs = 3, validation_data=(x_test, y_test1), shuffle = True, verbose = 0, batch_size=100)

predicciones = np.argmax(modelo.predict(x_test), axis=-1)

# matriz de confusion

matriz = confusion_matrix(y_test, np.argmax(modelo.predict(x_test), axis=-1))
print(matriz)

# verdaderos positivos

TP = np.diag(matriz) #objetner la diagonal
TP

#falsos positivos
FP = np.zeros(10)
for i in range(10):
  FP[i] = np.sum(matriz[:,i]) - TP[i]
print(FP)

# falsos negativos
FN = np.zeros(10)
for i in range(10):
  FN[i] = np.sum(matriz[i,:]) - TP[i]
print(FN)

# verdaderos negativos
TN = np.zeros(10)
for i in range(10):
  TN[i] = np.sum(matriz) - FN[i] - TP[i] - FP[i]
print(TN)

print(TP + FP + FN + TN)

# precision, recall, accuracy

P = np.sum(TP) / (np.sum(TP) + np.sum(FP))
R = np.sum(TP) / (np.sum(TP) + np.sum(FN))
A = (np.sum(TP) + np.sum(TN)) / (np.sum(TP) + np.sum(TN) + np.sum(FP) + np.sum(FN))
print('Precision: ', P)
print('Recall o sensibilidad: ', R)
print('Accuracy o exactitud: ', A)

